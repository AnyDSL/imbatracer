// HashGrid partial implementation in impala
// Try different array operations in impala 

extern "C" {
    fn abort() -> ();
}

// TODO cannot handle if grid.x / block.x > 65535, this is the limit for cc2.x 
fn iterate_acc(thread_per_block: i32, size: i32, body: fn(i32) -> ()) -> () {
    let block = (thread_per_block, 1, 1);
    let grid  = (ceilf32((size as f32) / (thread_per_block as f32)) as i32 * thread_per_block, 1, 1);
    
    with acc(acc_dev(), grid, block) {
        let i = acc_tidx() + acc_bidx() * acc_bdimx();
        if (i < size) {
            body(i);
        }
    }
}

// gpu generic reduction(f32 version)
// run user defined reduction on multiple "arrays"(e.g. float3) under user defined binary operation
// out_of_bound_value is used for the data whose index is out-of-bound(elements in the last block) based on the bin-op respectively (e.g. for + the value is 0)
// #input  = size * simd_stride
// #output = simd_stride
// TODO try use real SIMD functionality in impala
fn gpu_reduction(output_buf: Buffer, input: &[f32], size: i32, simd_stride: i32, op_idx: i32, bin_op_body: fn(&f32, f32, f32) -> ()) -> () {
    fn out_bound_val(op_idx: i32) -> f32 {
        if (op_idx == 0) { return(1e36f) }
        -1e36f
    }
    fn get_value_with_bound_check(array: &[f32], data_index: i32, array_length: i32) -> f32 {
        if (data_index < array_length) {
            return(array(data_index))
        } 
        out_bound_val(op_idx)
    }
    fn run_single_iteration(tid: i32, chunk_offset: i32, simd_stride: i32, mut sh_data: &[3][f32]) -> () {
        for cur_simd_idx in range(0, simd_stride) {
            let first_idx  = cur_simd_idx + simd_stride * tid;
            let second_idx = first_idx + simd_stride * chunk_offset;
            //sh_data(first_idx) = bin_op(sh_data(first_idx), sh_data(second_idx), op_idx);
            bin_op_body(&sh_data(first_idx) as &f32, sh_data(first_idx) as f32, sh_data(second_idx) as f32);
        }
    }
    fn bin_op(lhs: f32, rhs: f32, op_idx: i32) -> f32 {
        if (op_idx == 0) { return(min(lhs, rhs)) }
        max(lhs, rhs)
    }
    
    let input_length = size * simd_stride;
    let threads_per_block  = 256;
    let elements_per_block = threads_per_block * 2;
    let num_blocks         = ceilf32 ((size as f32) / (elements_per_block as f32)) as i32;
    let block = (threads_per_block, 1, 1);
    let grid  = (threads_per_block * num_blocks, 1, 1);

    let partial_output_buf     = acc_alloc(acc_dev(), num_blocks * simd_stride * sizeof[f32]());
    let mut ptr_partial_output = bitcast[&[f32]](partial_output_buf.data);
    
    with acc(acc_dev(), grid, block) {
        let bid = acc_bidx();
        let tid = acc_tidx();
        // multiplied by threads_per_block rather than elements_per_block due to the first reduction iteration will be done directly whild loading
        // i.e each thread load two elements perform operations and write the results to shared memory
        // 01.07.2016 seems the size of shared memory can only be constant at compile time, this make this function not robust and general
        let mut sh_data = reserve_shared[f32](threads_per_block * 3);
       
        // sequential addressing with stride offsets
        let first_start_idx  = ( tid + bid * elements_per_block ) * simd_stride;
        let second_start_idx = first_start_idx + threads_per_block * simd_stride;
        
        // load two chunks and store the results of the binary operations into shared memory
        for cur_simd_idx in range(0, simd_stride) {
            let first  = get_value_with_bound_check(input, first_start_idx + cur_simd_idx, input_length);
            let second = get_value_with_bound_check(input, second_start_idx + cur_simd_idx, input_length);
            sh_data(cur_simd_idx + simd_stride * tid) = bin_op(first, second, op_idx);
        }

        acc_barrier();

        // run reduction iterations
        for i in for_loop(threads_per_block >> 1,
                          |a: i32| -> i32 { a >> 1 },
                          |a: i32| -> bool { a > 0 })
        {
            if (tid < i) { run_single_iteration(tid, i, simd_stride, sh_data); }
            acc_barrier();
        }

        // write partial reduction chunk into partial_output
        if (tid == 0) {
            for cur_simd_idx in range(0, simd_stride) {
                ptr_partial_output(cur_simd_idx + simd_stride * bid) = sh_data(cur_simd_idx);
            }
        }
    }
    
    if (num_blocks > 1) {
        gpu_reduction(output_buf, ptr_partial_output, num_blocks, simd_stride, op_idx, bin_op_body);
    }
    else {
        copy(partial_output_buf, output_buf, simd_stride * sizeof[f32]());
    }
    
    //
    release(partial_output_buf);
}

// gpu scan (exclusive prefix-sum i32 version)
// based on report from Mark Harris: Parallel Prefix Sum (Scan) with CUDA
fn gpu_prefix_sum(input: &[i32], mut output: &[i32], size: i32) -> () {
    let log_num_banks = 5; // 32 banks
    fn conflict_free_offset(a: i32) -> i32 {
        a >> log_num_banks
    }
    fn get_value_with_bound_check(array: &[i32], i: i32, size: i32) -> i32 {
        if (i < size) { return (array(i)) }
        0
    }
    fn set_value_with_bound_check(mut array: &[i32], i: i32, size: i32, val: i32) -> () {
        if (i < size) { array(i) = val; }
    }

    let threads_per_block  = 256;
    let elements_per_block = threads_per_block * 2;
    let num_blocks         = ceilf32 ((size as f32) / (elements_per_block as f32)) as i32;
    let num_blocks_floor   = floorf32((size as f32) / (elements_per_block as f32)) as i32;
    let block = (threads_per_block, 1, 1);
    let grid  = (threads_per_block * num_blocks, 1, 1);

    let auxi_buf     = acc_alloc(acc_dev(), num_blocks * sizeof[i32]());
    let auxi_pfs_buf = acc_alloc(acc_dev(), num_blocks * sizeof[i32]());
    let mut ptr_auxi = bitcast[&[i32]](auxi_buf.data);
    let ptr_auxi_pfs = bitcast[&[i32]](auxi_pfs_buf.data);
    
    with acc(acc_dev(), grid, block) {
        let bid = acc_bidx();
        let tid = acc_tidx();
        let mut sh_pfs = reserve_shared[i32](elements_per_block * 2); // because of padding, simply reserve twice of the original size
        
        // for all blocks except the last one
        // run work-efficient scan introduced by harris
        if (bid < num_blocks) {
            let mut offset = 1;
            let ai = tid;
            let bi = tid + (elements_per_block >> 1);
            sh_pfs(ai + conflict_free_offset(ai)) = get_value_with_bound_check(input, ai + bid * elements_per_block, size);
            sh_pfs(bi + conflict_free_offset(bi)) = get_value_with_bound_check(input, bi + bid * elements_per_block, size);
            
            // up-sweep
            for i in for_loop(elements_per_block >> 1, 
                              |a: i32| -> i32  { a >> 1 },
                              |a: i32| -> bool { a > 0 }) 
            {
                acc_barrier();
                if (tid < i) {
                    let mut ai2  = offset * ( 2 * tid + 1) - 1;
                    let mut bi2  = offset * ( 2 * tid + 2) - 1;
                    ai2 += conflict_free_offset(ai2);
                    bi2 += conflict_free_offset(bi2);
                    sh_pfs(bi2) += sh_pfs(ai2);
                }
                offset *= 2;
            }   
            
            //
            if (tid == 0) {
                let tmp = elements_per_block - 1;
                let mut ptr_last_element = &(sh_pfs(tmp + conflict_free_offset(tmp)));

                // before zeroing the last element of each block, initialize auxi array first
                ptr_auxi(bid) = *ptr_last_element;
                
                // zero last element of each block to get a exclusive prefix-sum eventually
                *ptr_last_element = 0;
            }

            // down-sweep
            for i in for_loop(1,
                              |a: i32| -> i32  { a * 2 },
                              |a: i32| -> bool { a < elements_per_block })
            {
                offset >>= 1;
                acc_barrier();
                if (tid < i) {
                    let mut ai2  = offset * ( 2 * tid + 1) - 1;
                    let mut bi2  = offset * ( 2 * tid + 2) - 1;
                    ai2 += conflict_free_offset(ai2);
                    bi2 += conflict_free_offset(bi2);
                    let t        = sh_pfs(ai2);
                    sh_pfs(ai2)  = sh_pfs(bi2);
                    sh_pfs(bi2) += t;
                }
            }
            acc_barrier();
            set_value_with_bound_check(output, ai + bid * elements_per_block, size, sh_pfs(ai + conflict_free_offset(ai)));
            set_value_with_bound_check(output, bi + bid * elements_per_block, size, sh_pfs(bi + conflict_free_offset(bi)));
        }
    }

    if (num_blocks > 1) {
        // pre-scan auxi array
        gpu_prefix_sum(ptr_auxi, ptr_auxi_pfs, num_blocks); 

        // uniform add the auxi array for the respective block elements in output array
        for i in iterate_acc(elements_per_block, size) {
            let bid = acc_bidx();
            output(i) += ptr_auxi_pfs(bid);
        }
    }

    //
    release(auxi_buf);
    release(auxi_pfs_buf);
}

// gpu stream compaction
fn gpu_stream_compaction(sum: &[i32], mask: &[i32], input: &[i32], mut output: &[i32], threads_per_block: i32, size: i32) -> () {
    for i in iterate_acc(threads_per_block, size) {
        if (mask(i) == 1) {
            output(sum(i)) = input(i);
        }
    } 
} 

// Interfaces
extern fn build_hashgrid(info: &RawDataInfo, photon_cnt: i32, cell_size: i32, rad: f32) -> &PhotonHashGrid {
    let indices_buf   = alloc_cpu(photon_cnt * sizeof[i32]());
    let cell_ends_buf = alloc_cpu(cell_size * sizeof[i32]());
    let photons_buf   = alloc_cpu(photon_cnt * 3 * sizeof[f32]());
    let photons_buf2  = acc_alloc(acc_dev(), photon_cnt * 3 * sizeof[f32]());

    let mut ptr_photons = bitcast[&[f32]](photons_buf.data);
    for i in range (0, photon_cnt) {
        for j in @unroll(0, 3) {
            ptr_photons(3 * i + j) = (*info).begin(i * (*info).stride + j); 
        }
    }

    let mut hg = ~PhotonHashGrid {
        radius         : rad,
        radius_sqr     : rad * rad,
        cell_size      : rad * 2.f,
        inv_cell_size  : 1.f / (rad * 2.f),
        photons_size   : photon_cnt,
        indices_size   : photon_cnt,
        cell_ends_size : cell_size,
        bbox_min       : Float3 { x:1e36f,  y:1e36f,  z:1e36f },
        bbox_max       : Float3 { x:-1e36f, y:-1e36f, z:-1e36f },
        raw_data_info  : RawDataInfo {
            begin : (*info).begin,
            stride: (*info).stride
        },
        indices        : bitcast[&[i32]](indices_buf.data),
        cell_ends      : bitcast[&[i32]](cell_ends_buf.data),
        photons        : bitcast[&[f32]](photons_buf.data),     
        neighbor       : ~[8 : i32],

        host_photons_buf        : photons_buf,
        host_indices_buf        : indices_buf,
        host_cell_ends_buf      : cell_ends_buf,
        host_result_buf         : alloc_cpu(photon_cnt * sizeof[i32]()),
        
        dev_photons_buf         : photons_buf2, 
        dev_indices_buf         : acc_alloc(acc_dev(), photon_cnt * sizeof[i32]()),
        dev_cell_ends_buf       : acc_alloc(acc_dev(), cell_size  * sizeof[i32]()),
        dev_mask_buf            : acc_alloc(acc_dev(), photon_cnt * sizeof[i32]()),
        dev_pfs_buf             : acc_alloc(acc_dev(), photon_cnt * sizeof[i32]())
    };

    copy(hg.host_photons_buf, hg.dev_photons_buf, photon_cnt * 3 * sizeof[f32]());

    let dev_cnt_buf  = acc_alloc(acc_dev(), cell_size * sizeof[i32]());
    let dev_hash_buf = acc_alloc(acc_dev(), photon_cnt * sizeof[i32]());
    let dev_pos_buf  = acc_alloc(acc_dev(), photon_cnt * sizeof[i32]());

    let ptr_dev_photons       = bitcast[&[f32]](hg.dev_photons_buf.data);
    let mut ptr_dev_cell_ends = bitcast[&[i32]](hg.dev_cell_ends_buf.data); 
    let mut ptr_dev_indices   = bitcast[&[i32]](hg.dev_indices_buf.data);
    let mut ptr_cnt  = bitcast[&[i32]](dev_cnt_buf.data);
    let mut ptr_hash = bitcast[&[i32]](dev_hash_buf.data);
    let mut ptr_pos  = bitcast[&[i32]](dev_pos_buf.data);

    //
    let host_bbox_min_buf = alloc_cpu(3 * sizeof[f32]());
    let host_bbox_max_buf = alloc_cpu(3 * sizeof[f32]());
    for mut ptr_out, lhs, rhs in gpu_reduction(host_bbox_min_buf, ptr_dev_photons, hg.photons_size, 3, 0) {
        *ptr_out = min(lhs, rhs);
    }
    for mut ptr_out, lhs, rhs in gpu_reduction(host_bbox_max_buf, ptr_dev_photons, hg.photons_size, 3, 1) {
        *ptr_out = max(lhs, rhs);
    }
    
    let ptr_bbox_min = bitcast[&[f32]](host_bbox_min_buf.data);
    let ptr_bbox_max = bitcast[&[f32]](host_bbox_max_buf.data);
    hg.bbox_min.x = ptr_bbox_min(0);
    hg.bbox_min.y = ptr_bbox_min(1);
    hg.bbox_min.z = ptr_bbox_min(2);
    hg.bbox_max.x = ptr_bbox_max(0);
    hg.bbox_max.y = ptr_bbox_max(1);
    hg.bbox_max.z = ptr_bbox_max(2);
    release(host_bbox_min_buf);
    release(host_bbox_max_buf);
    //
    
    let threads_per_block = 256;
    let bbox_min = hg.bbox_min;
    let inv_cell_size = hg.inv_cell_size;
    
    for i in iterate_acc(threads_per_block, cell_size) {
        ptr_cnt(i) = 0;
    }

    for i in iterate_acc(threads_per_block, photon_cnt) {
        let pos = Float3 {
            x : ptr_dev_photons(3 * i),
            y : ptr_dev_photons(3 * i + 1),
            z : ptr_dev_photons(3 * i + 2)
        };
        let hash  = cell_index(pos, bbox_min, inv_cell_size, cell_size);
        ptr_hash(i) = hash;
        ptr_pos(i) = atomic_add(&(ptr_cnt(hash)), 1); 
    }

    gpu_prefix_sum(ptr_cnt, ptr_dev_cell_ends, cell_size);

    for i in iterate_acc(threads_per_block, photon_cnt) {
        ptr_dev_indices(ptr_pos(i) + ptr_dev_cell_ends(ptr_hash(i))) = i;
    }

    for i in iterate_acc(threads_per_block, cell_size) {
        ptr_dev_cell_ends(i) += ptr_cnt(i);
    }

    // copy
    copy(hg.dev_indices_buf, hg.host_indices_buf, photon_cnt * sizeof[i32]());
    copy(hg.dev_cell_ends_buf, hg.host_cell_ends_buf, cell_size * sizeof[i32]());

    // release
    release(dev_cnt_buf);
    release(dev_hash_buf);
    release(dev_pos_buf);

    hg
}

extern fn query_hashgrid(mut hg: &PhotonHashGrid, x: f32, y: f32, z: f32) -> &QueryResult {
    let size = (*hg).photons_size;
    
    let query_pos = Float3 {
        x : x,
        y : y,
        z : z
    };
    

    let dist_min = sub(query_pos, (*hg).bbox_min);
    let dist_max = sub((*hg).bbox_max, query_pos);

    if (has_negative(dist_min) || has_negative(dist_max)) { 
        return(~QueryResult {
                   size   : 0,
                   data   : bitcast[&[i32]]((*hg).host_result_buf.data)
               }) 
    }
    
    let cell = scalar((*hg).inv_cell_size, dist_min);

    let coord = Float3 {
        x : floorf32(cell.x),
        y : floorf32(cell.y),
        z : floorf32(cell.z)
    };

    let px = coord.x as i32;
    let py = coord.y as i32;
    let pz = coord.z as i32;

    let frac_coord = sub(cell, coord);

    let dx = if (frac_coord.x < 0.5f) { -1 } else { 1 };
    let dy = if (frac_coord.y < 0.5f) { -1 } else { 1 };
    let dz = if (frac_coord.z < 0.5f) { -1 } else { 1 };

    let pxo = px + dx;
    let pyo = py + dy;
    let pzo = pz + dz;

    let elements_per_block = 512;
    let threads_per_block  = 256;
    let num_blocks         = size / elements_per_block;
    let cell_ends_size     = (*hg).cell_ends_size;
    let inv_cell_size      = (*hg).inv_cell_size;
    let radius_sqr         = (*hg).radius_sqr;
    let ptr_photons  = bitcast[&[f32]]((*hg).dev_photons_buf.data); 
    let ptr_indices  = bitcast[&[i32]]((*hg).dev_indices_buf.data); 
    let mut ptr_mask = bitcast[&[i32]]((*hg).dev_mask_buf.data);
    let ptr_pfs      = bitcast[&[i32]]((*hg).dev_pfs_buf.data);

    for i in @unroll(0, 8) {
        let z = if ((i & 1) != 0) { pzo } else { pz };
        let y = if ((i & 2) != 0) { pyo } else { py };
        let x = if ((i & 4) != 0) { pxo } else { px };

        //let interval = cell_range(hash_func(x as u32, y as u32, z as u32, (*hg).cell_ends_size as u32), (*hg).cell_ends);  
        (*hg).neighbor(i) = hash_func(x as u32, y as u32, z as u32, (*hg).cell_ends_size as u32);  
        //let start = arr.size;
        //let mut curCnt = 0;

        //for j in $range(interval.x, interval.y) {
        //    let idx = (*hg).indices(j);
        //    let pos = toFloat3(hg, idx);
        //    let distSqr = sqr(sub(query_pos, pos));
        //    if (distSqr <= (*hg).radius_sqr) {
        //        arr.data(start + curCnt) = idx;
        //        curCnt++;
        //    }
        //}

        //arr.size += curCnt;
    }

    let n0 = (*hg).neighbor(0);
    let n1 = (*hg).neighbor(1);
    let n2 = (*hg).neighbor(2);
    let n3 = (*hg).neighbor(3);
    let n4 = (*hg).neighbor(4);
    let n5 = (*hg).neighbor(5);
    let n6 = (*hg).neighbor(6);
    let n7 = (*hg).neighbor(7);

    let bbox_min_x = (*hg).bbox_min.x;
    let bbox_min_y = (*hg).bbox_min.y;
    let bbox_min_z = (*hg).bbox_min.z;

    // TODO: all codes above should be dealt in c++ and passed in as args

    // kernel 1: filtering based on neighbor grids and radius
    for i in iterate_acc(threads_per_block, size) {
        ptr_mask(i) = 0;
        let idx = ptr_indices(i);
        let x2  = ptr_photons(3 * idx);
        let y2  = ptr_photons(3 * idx + 1);
        let z2  = ptr_photons(3 * idx + 2);
        let x3  = floorf32(inv_cell_size * (x2 - bbox_min_x));
        let y3  = floorf32(inv_cell_size * (y2 - bbox_min_y));
        let z3  = floorf32(inv_cell_size * (z2 - bbox_min_z));
        let h   = hash_func(x3 as u32, y3 as u32, z3 as u32, cell_ends_size as u32);  
        
        // TODO: neighbor filtering should be also moved into c++ and passed in as the initial mask

        if (h == n0) { ptr_mask(i) = 1; }
        else if (h == n1) { ptr_mask(i) = 1; }
        else if (h == n2) { ptr_mask(i) = 1; }
        else if (h == n3) { ptr_mask(i) = 1; }
        else if (h == n4) { ptr_mask(i) = 1; }
        else if (h == n5) { ptr_mask(i) = 1; }
        else if (h == n6) { ptr_mask(i) = 1; }
        else if (h == n7) { ptr_mask(i) = 1; }
        if (ptr_mask(i) == 1) {
            let dx = x - x2;
            let dy = y - y2;
            let dz = z - z2;
            let distSqr = dx * dx + dy * dy + dz * dz;
            if (distSqr > radius_sqr) {
                ptr_mask(i) = 0;
            }
        }
    }

    // kernel 2: scan(prefix-sum)
    //let mut start = thorin_get_kernel_time() as i32;
    gpu_prefix_sum(ptr_mask, ptr_pfs, size);
    //let mut end   = thorin_get_kernel_time() as i32;
    //print_int(end - start);
    //print_char('\n');
    
    // kernel 3: stream compaction
    // try to get the size of the compacted array, is this the only way to do it?
    let tmp_buf1 = alloc_cpu(sizeof[i32]());
    let tmp_buf2 = alloc_cpu(sizeof[i32]());
    copy_offset((*hg).dev_pfs_buf , (size - 1) * sizeof[i32](), tmp_buf1, 0, sizeof[i32]());
    copy_offset((*hg).dev_mask_buf, (size - 1) * sizeof[i32](), tmp_buf2, 0, sizeof[i32]());
    let sc_size = bitcast[&[i32]](tmp_buf1.data)(0) + bitcast[&[i32]](tmp_buf2.data)(0);   
    release(tmp_buf1);
    release(tmp_buf2);
    
    let dev_sc_buf  = acc_alloc(acc_dev(), sc_size * sizeof[i32]());

    //start = thorin_get_kernel_time() as i32;
    gpu_stream_compaction(ptr_pfs, ptr_mask, ptr_indices, bitcast[&[i32]](dev_sc_buf.data), 256, size);
    //end   = thorin_get_kernel_time() as i32;
    //print_int(end - start);
    //print_char('\n');
    
    copy(dev_sc_buf, (*hg).host_result_buf, sc_size * sizeof[i32]());
    release(dev_sc_buf);
//print_int(sc_size);
//print_char('\n');
    //
    ~QueryResult {
        size   : sc_size,
        data   : bitcast[&[i32]]((*hg).host_result_buf.data)
    }
}

extern fn batch_query_hashgrid(hg: &PhotonHashGrid, query_poses: &[f32], size: i32) -> &BatchQueryResult {
    let THREADS_PER_BLOCK = 1024;
    //
    let radius_sqr     = (*hg).radius_sqr;
    let cell_ends_size = (*hg).cell_ends_size;
    let inv_cell_size  = (*hg).inv_cell_size;
    let bbox_min_x = (*hg).bbox_min.x;
    let bbox_min_y = (*hg).bbox_min.y;
    let bbox_min_z = (*hg).bbox_min.z;
    let bbox_max_x = (*hg).bbox_max.x;
    let bbox_max_y = (*hg).bbox_max.y;
    let bbox_max_z = (*hg).bbox_max.z;
    let ptr_cell_ends = bitcast[&[i32]]((*hg).dev_cell_ends_buf.data); 
    let ptr_indices   = bitcast[&[i32]]((*hg).dev_indices_buf.data); 
    let ptr_photons   = bitcast[&[f32]]((*hg).dev_photons_buf.data); 
    //
    let host_query_poses_buf = alloc_cpu(size * 3 * sizeof[f32]());
    let dev_query_poses_buf  = acc_alloc(acc_dev(), size * 3 * sizeof[f32]());
    let mut ptr_host_query_poses = bitcast[&[f32]](host_query_poses_buf.data);
    let ptr_dev_query_poses      = bitcast[&[f32]](dev_query_poses_buf.data);
    for i in range(0, size) {
        ptr_host_query_poses(i * 3)     = query_poses(i * 3); 
        ptr_host_query_poses(i * 3 + 1) = query_poses(i * 3 + 1); 
        ptr_host_query_poses(i * 3 + 2) = query_poses(i * 3 + 2); 
    }
    copy(host_query_poses_buf, dev_query_poses_buf, size * 3 * sizeof[f32]());
    //
    let dev_batch_count_buf     = acc_alloc(acc_dev(), size * sizeof[i32]());
    let dev_batch_count_pfs_buf = acc_alloc(acc_dev(), size * sizeof[i32]()); 
    let mut ptr_batch_count     = bitcast[&[i32]](dev_batch_count_buf.data);
    let     ptr_batch_count_pfs = bitcast[&[i32]](dev_batch_count_pfs_buf.data);

    // kernel 1: count number of photons in neighbor cells for each query position
    for i in iterate_acc(THREADS_PER_BLOCK, size) {
        let mut cnt = 0;
        let qx = ptr_dev_query_poses(i * 3);
        let qy = ptr_dev_query_poses(i * 3 + 1);
        let qz = ptr_dev_query_poses(i * 3 + 2);
        if (!is_query_out_of_bbox(qx, qy, qz, bbox_min_x, bbox_min_y, bbox_min_z, bbox_max_x, bbox_max_y, bbox_max_z)) {
            for x, y, z in iterate_neighbor_cells(qx, qy, qz, bbox_min_x, bbox_min_y, bbox_min_z, inv_cell_size)
            {
                let cell_idx = hash_func(x as u32, y as u32, z as u32, cell_ends_size as u32);
                let mut start = 0;
                let end = ptr_cell_ends(cell_idx);
                if ( cell_idx > 0) {
                    start = ptr_cell_ends(cell_idx - 1);
                }
                cnt += end - start;
            }
        }
        ptr_batch_count(i) = cnt;
    }

    // kernel 2: prefix_sum for photon counts for all queries
    @gpu_prefix_sum(ptr_batch_count, ptr_batch_count_pfs, size);

    // compute size of batch photons and alloc memory on device respectively 
    let host_last_batch_count_buf     = alloc_cpu(sizeof[i32]());
    let host_last_batch_count_pfs_buf = alloc_cpu(sizeof[i32]());
    copy_offset(dev_batch_count_buf,     (size - 1) * sizeof[i32](), host_last_batch_count_buf,     0, sizeof[i32]());
    copy_offset(dev_batch_count_pfs_buf, (size - 1) * sizeof[i32](), host_last_batch_count_pfs_buf, 0, sizeof[i32]());
    let batch_photons_count = bitcast[&[i32]](host_last_batch_count_buf.data)(0)
                            + bitcast[&[i32]](host_last_batch_count_pfs_buf.data)(0);
    release(host_last_batch_count_buf);
    release(host_last_batch_count_pfs_buf);
    // DEBUG ONLY
    if (batch_photons_count > 100000000) {
        print_string("batch_photons_count = ");
        print_int(batch_photons_count);
        print_char('\n');
    }
    //
    let dev_batch_indices_buf   = acc_alloc(acc_dev(), batch_photons_count * sizeof[i32]());
    let dev_isect_indices_buf   = acc_alloc(acc_dev(), batch_photons_count * sizeof[i32]());
    let dev_batch_mask_buf      = acc_alloc(acc_dev(), batch_photons_count * sizeof[i32]());
    let dev_batch_mask_pfs_buf  = acc_alloc(acc_dev(), batch_photons_count * sizeof[i32]());
    let mut ptr_batch_indices   = bitcast[&[i32]](dev_batch_indices_buf.data); 
    let mut ptr_isect_indices   = bitcast[&[i32]](dev_isect_indices_buf.data); 
    let mut ptr_batch_mask      = bitcast[&[i32]](dev_batch_mask_buf.data); 
    let     ptr_batch_mask_pfs  = bitcast[&[i32]](dev_batch_mask_pfs_buf.data); 

    // kernel 3: create batch indices for all photons
    for i in iterate_acc(THREADS_PER_BLOCK, size) {
        let mut cnt = 0;
        let offset  = ptr_batch_count_pfs(i);
        let qx = ptr_dev_query_poses(i * 3);
        let qy = ptr_dev_query_poses(i * 3 + 1);
        let qz = ptr_dev_query_poses(i * 3 + 2);
        if (ptr_batch_count(i) > 0) {
            for x, y, z in iterate_neighbor_cells(qx, qy, qz, bbox_min_x, bbox_min_y, bbox_min_z, inv_cell_size) {
                let cell_idx = hash_func(x as u32, y as u32, z as u32, cell_ends_size as u32);
                let mut start = 0;
                let end = ptr_cell_ends(cell_idx);
                if ( cell_idx > 0) {
                    start = ptr_cell_ends(cell_idx - 1);
                }
                for j in range(start, end) {
                    ptr_batch_indices(cnt + offset) = ptr_indices(j);
                    ptr_isect_indices(cnt + offset) = i;
                    ++cnt;
                }
            }
        }
        // reset to 0 for output usage
        ptr_batch_count(i) = 0;
    }

    // kernel 4: radius based filtering
    for i in iterate_acc(THREADS_PER_BLOCK, batch_photons_count) {
        ptr_batch_mask(i) = 0;
        let isect_idx  = ptr_isect_indices(i);
        let photon_idx = ptr_batch_indices(i);
        let qx = ptr_dev_query_poses(isect_idx * 3);
        let qy = ptr_dev_query_poses(isect_idx * 3 + 1);
        let qz = ptr_dev_query_poses(isect_idx * 3 + 2);
        let px = ptr_photons(photon_idx * 3); 
        let py = ptr_photons(photon_idx * 3 + 1); 
        let pz = ptr_photons(photon_idx * 3 + 2);
        let dx = qx - px;
        let dy = qy - py;
        let dz = qz - pz;
        let dist_sqr = dx * dx + dy * dy + dz * dz;
        if (dist_sqr <= radius_sqr) {
            ptr_batch_mask(i) = 1;
        }
    }

    // kernel 5: prefix-sum on mask
    @gpu_prefix_sum(ptr_batch_mask, ptr_batch_mask_pfs, batch_photons_count);

    // alloc memory for outputs
    let host_last_batch_mask_buf     = alloc_cpu(sizeof[i32]());
    let host_last_batch_mask_pfs_buf = alloc_cpu(sizeof[i32]());
    copy_offset(dev_batch_mask_buf,     (batch_photons_count - 1) * sizeof[i32](), host_last_batch_mask_buf,     0, sizeof[i32]());
    copy_offset(dev_batch_mask_pfs_buf, (batch_photons_count - 1) * sizeof[i32](), host_last_batch_mask_pfs_buf, 0, sizeof[i32]());
    let output_size = bitcast[&[i32]](host_last_batch_mask_buf.data)(0) + bitcast[&[i32]](host_last_batch_mask_pfs_buf.data)(0);
    release(host_last_batch_mask_buf);
    release(host_last_batch_mask_pfs_buf);

    let dev_output_indices_buf = acc_alloc(acc_dev(), output_size * sizeof[i32]());
    let mut ptr_output_indices = bitcast[&[i32]](dev_output_indices_buf.data); 

    // kernel 6: stream compaction
    for i in iterate_acc(THREADS_PER_BLOCK, batch_photons_count) {
        if (ptr_batch_mask(i) == 1) {
            ptr_output_indices(ptr_batch_mask_pfs(i)) = ptr_batch_indices(i);
            atomic_add(&ptr_batch_count(ptr_isect_indices(i)), 1);
        }
    }

    // kernel 7: recompute prefix-sum of ptr_batch_count as part of the output data
    gpu_prefix_sum(ptr_batch_count, ptr_batch_count_pfs, size);
    
    // copy output from device to host
    let host_output_indices_buf = alloc_cpu(output_size * sizeof[i32]());
    let host_output_offsets_buf = alloc_cpu(size * sizeof[i32]());
    copy(dev_output_indices_buf,  host_output_indices_buf, output_size * sizeof[i32]());
    copy(dev_batch_count_pfs_buf, host_output_offsets_buf, size * sizeof[i32]());

    // release buffer
    release(host_query_poses_buf);
    release(dev_query_poses_buf);
    release(dev_batch_count_buf);
    release(dev_batch_count_pfs_buf);
    release(dev_batch_indices_buf);
    release(dev_isect_indices_buf);
    release(dev_batch_mask_buf);
    release(dev_batch_mask_pfs_buf);
    release(dev_output_indices_buf);

    ~BatchQueryResult {
        size : output_size,
        indices : bitcast[&[i32]](host_output_indices_buf.data),
        offsets : bitcast[&[i32]](host_output_offsets_buf.data),
        indices_buf : host_output_indices_buf,
        offsets_buf : host_output_offsets_buf
    }
}

